{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kA6uPNS0I4N8"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai\n",
        "!pip install -qU pinecone\n",
        "!pip install -qU google-generativeai\n",
        "!pip install -qU sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get(\"PINECONE_API_KEY\")\n",
        "os.environ[\"PINECONE_ENV\"] = userdata.get(\"PINECONE_ENV\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n"
      ],
      "metadata": {
        "id": "sAVoH7POI7YZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "index_name = \"digital-gold-index\"\n",
        "if index_name in pc.list_indexes().names():\n",
        "    pc.delete_index(index_name)\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=384,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=os.environ[\"PINECONE_ENV\"]\n",
        "    )\n",
        ")\n",
        "\n",
        "index = pc.Index(index_name)\n"
      ],
      "metadata": {
        "id": "xmCM1M4gJMlX"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/business_doc.txt\", \"r\") as f:\n",
        "    document = f.read()\n",
        "chunks = [chunk.strip() for chunk in document.split(\"\\n\\n\") if chunk.strip()]\n"
      ],
      "metadata": {
        "id": "JlnlCwLeJdkz"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, chunk in enumerate(chunks):\n",
        "    embedding = embedding_model.encode(chunk).tolist()\n",
        "    index.upsert([(f\"doc-{i}\", embedding, {\"text\": chunk})])\n"
      ],
      "metadata": {
        "id": "n9eB1w8ORLPG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def search_pinecone(query: str, top_k: int = 3) -> List[str]:\n",
        "    query_embedding = embedding_model.encode(query).tolist()\n",
        "    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
        "    return [match[\"metadata\"][\"text\"] for match in results[\"matches\"]]\n",
        "\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# # Embed and upload chunks\n",
        "# for i, chunk in enumerate(chunks):\n",
        "#     embedding = embedding_model.encode(chunk).tolist()\n",
        "#     index.upsert([(f\"doc-{i}\", embedding, {\"text\": chunk})])\n"
      ],
      "metadata": {
        "id": "80YIj-khJ1am"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "\n",
        "def generate_answer(query: str) -> str:\n",
        "    context = \"\\n\".join(search_pinecone(query))\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a professional virtual assistant designed specifically to help users with business-related queries only.\n",
        "\n",
        "Respond strictly to questions related to business operations, services, products, policies, pricing, support, transactions, hours, or anything clearly tied to a business context.\n",
        "\n",
        "If the user asks a question that is personal, casual, philosophical, unrelated to business, or off-topic, politely respond:\n",
        "\"Sorry, I can only assist with business-related questions.\"\n",
        "\n",
        "Use only the following context to answer the question:\n",
        "{context}\n",
        "\n",
        "Now, based on the above instructions and the provided context, answer the user's question.\n",
        "\n",
        "User's Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------\n",
        "# from typing import List\n",
        "\n",
        "# def search_pinecone(query: str, top_k: int = 3) -> List[str]:\n",
        "#     query_embedding = embedding_model.encode(query).tolist()\n",
        "#     results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)\n",
        "#     return [match[\"metadata\"][\"text\"] for match in results[\"matches\"]]\n",
        "\n",
        "# def generate_answer(query: str) -> str:\n",
        "#     context = \"\\n\".join(search_pinecone(query))\n",
        "\n",
        "#     messages = [\n",
        "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant for a digital gold business.\"},\n",
        "#         {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\"}\n",
        "#     ]\n",
        "\n",
        "#     response = client.chat.completions.create(\n",
        "#         model=\"gpt-3.5-turbo\",\n",
        "#         messages=messages\n",
        "#     )\n",
        "\n",
        "#     return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "YUJhXCDrQt6d"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input(\"Ask your business-related question: \")\n",
        "answer = generate_answer(query)\n",
        "print(\"\\nAnswer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "UktYEm6xQw4p",
        "outputId": "4329c4ae-40ed-4c24-afab-dda06ce59a15"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask your business-related question: How do I sell my gold?\n",
            "\n",
            "Answer: Sell any amount of your digital gold instantly via our platform. Proceeds are credited to your bank account within 24 hours.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}